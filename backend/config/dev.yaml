# WebAgent Development Environment Configuration

# API settings
api:
  version: "v1"
  prefix: "/api/v1"
  debug_mode: true
  host: "0.0.0.0"
  port: 8000

# CORS settings
cors:
  origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"
  allow_credentials: true
  allow_methods: ["*"]
  allow_headers: ["*"]

# Database settings
database:
  vector_db:
    path: "./data/vectordb"
    embedding_dimension: 1536
    distance_metric: "cosine"
  redis:
    host: "localhost"
    port: 6379
    db: 0
    password: null
    key_prefix: "webagent:dev:"

# LLM settings
llm:
  # Provider options: "openai", "together", "self"
  provider: "together"
  default_model: "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
  temperature: 0.7
  timeout_seconds: 60
  max_tokens: 4096
  retry_attempts: 3
  batch_size: 10
  # Self-hosted LLM specific settings (used when provider is "self")
  self_hosted_url: "http://localhost:8080"
  model_path: "/path/to/your/model"  # Only used when starting the self-hosted service

# LangSmith settings for observability
langsmith:
  project_name: "webagent-research-dev"
  tracing_enabled: true
  log_level: "DEBUG"

# Web search settings
web_search:
  provider: "tavily"
  search_depth: "basic"  # basic or comprehensive
  max_results: 10
  include_domains: []
  exclude_domains: []
  timeout_seconds: 30

# Task management
tasks:
  max_concurrent: 10
  result_ttl_minutes: 60
  history_ttl_days: 7
  max_retries: 3

# Agents configuration
agents:
  supervisor:
    model: "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
    temperature: 0.2
  web_research:
    model: "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
    temperature: 0.3
    max_searches_per_task: 5
  internal_research:
    model: "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
    temperature: 0.1
    max_documents: 20
  senior_research:
    model: "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
    temperature: 0.2
    max_follow_up_questions: 3
  data_analysis:
    model: "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
    temperature: 0.1
  coding:
    model: "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
    temperature: 0.0
    allowed_modules: ["matplotlib", "seaborn", "numpy", "pandas"]
    timeout_seconds: 30
  team_manager:
    model: "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
    temperature: 0.4

# Logging configuration
logging:
  level: "DEBUG"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_to_file: true
  log_file: "./logs/webagent-dev.log"
  rotate_logs: true
  max_file_size_mb: 10
  backup_count: 5

# Security settings
security:
  token_expire_minutes: 30
  algorithm: "HS256"
  password_min_length: 8 